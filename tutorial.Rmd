---
title: "Reading data from the web"
author: "Reka Solymosi"
date: "25 July 2016"
output: html_document
---

#Quick demo of how to read data from the web using R
##Special focus: how to get around your corporate firewall

#The TFL APIs

You can access the Transport for London Unified API page [here](https://api.tfl.gov.uk/). In order to be able to request data, you will need to register, and get an app id and key. You can do so [here](https://api-portal.tfl.gov.uk/login). Once you have that, you can have a look through the unified API site for all the different data streams. 

There are also a series of blog posts that tell you more about the API [here](https://blog.tfl.gov.uk/2015/10/01/tfl-unified-api-part-1-introduction/). 

All you have to do is make a call using a URL. For this  you need to know the structure for the request, which you can see on the unified API website, your search parameters, and your app id and key. 

So for example to build a call, to get accident data for the year 2015, this is how you would structure your call: 

```{r}

year <- "2015"
myAppId <- "0f1de667"
myAppKey <- "6c47ac149cfec9ccb6bf32b953a85dd3"

requestUrl <- paste0("https://api.tfl.gov.uk/AccidentStats/", year, "?app_id=", myAppId, "&app_key=", myAppKey)

```

And the resulting `requestUrl` is what you need to return your accidents data for 2015. This returns data in a JSON format. If you wanted to have a look at what that looks like raw, then just copypasta the url you just created into a web browser. It will just look super messy. 

##Reading data from APIs
There are loads of different ways to do this, but here I just use the package JSONLITE. If you don't have it already you will need to install and load the package:

```{r, eval=FALSE}

install.packages("jsonlite")

```
```{r, message=FALSE, warning=FALSE}

library(jsonlite)

```
This package is essentailly "a fast JSON parser and generator optimized for statistical data
and the web". And in ideal conditions it is very easy to use it to parse in JSON data returned by the TFL API call. In ideal conditions it should take literally one line to parse the data: 

```{r, eval=FALSE}

d = fromJSON(requestUrl)

```

However if you try this on a corporate computer you might receive a connection error, because firewall. 

Fortunately, there is a workaround. If you use `readLines` first to read in the returned text from the URL, and then within R parse that line, then it works alright. 

```{r}

l = readLines(requestUrl, encoding="UTF-8", warn=FALSE)
d = fromJSON(l)

```

And that should work!

Your data should look something like this:

```{r, echo=FALSE, message=FALSE, warning=FALSE}

library(knitr)

kable(head(data.frame(lapply(as.data.frame(d), as.character), stringsAsFactors=FALSE), n=3), format = "markdown")

```

You can now do something with this data. Like make a map:

```{r, echo=FALSE, message=FALSE, warning=FALSE}

library(leaflet)
d$vehMode <- lapply(d$vehicles, `[[`, "type")
content <- paste0("<b>Severity:</b>",
  d$severity,
  "<br>",
  "<b>Vehicles involved were:</b>",
  unique(d$vehMode))

leaflet(data = d) %>% addTiles() %>%
  addMarkers(~lon, ~lat, popup = content,  clusterOptions = markerClusterOptions())

```






